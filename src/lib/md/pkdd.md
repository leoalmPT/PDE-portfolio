The paper "Understanding What Federated Learning Models Learn: A Comparative Study with Traditional Models" has been submitted. This paper was developed at Instituto de Telecomunicações, and the main author is my colleague, Rafael Teixeira.

<br>

This paper explores the impact of Federated Learning (FL) on model explainability by comparing FL models with traditional single-host trained models. It assesses the correlation between feature importance determined by two XAI methods—Permutation Importance (PI) and Partial Dependence Variance (PDV)—across various FL strategies and a conventional training approach. Four datasets were used, focusing on network slicing and intrusion detection tasks relevant for 5G and 6G networks. The results revealed significant differences in feature importance despite similar performance metrics, emphasizing the necessity of multi-method explainability for FL models.

<br>

In this collaborative effort, I contributed to devoloping the experimental setup, including FL and XAI, and running the experiments to evaluate the models' explainability.

